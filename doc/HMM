# 隐马尔科夫（HMM）

## 简介

隐马尔科夫可以用于序列标注问题，比如分词过程，假设分词按照BEOS过程，在这里，隐马尔科夫就是从隐马尔科夫模型生成隐藏状态序列

## 基本参数

- 初始概率向量π：就是我一开始分别是B、E、O、S的概率如{”B“:0.6,“S”:0.4}

- 状态转移概率A：就是我从其中一个隐状态到达另一个隐藏状态的概率，如：当前为B，则下一个字符是E的概率是0.8

- 观测概率矩阵B：就是从当前隐藏状态生成当前的概率，也叫发射概率：如当前是B，生成字符“我”的概率是0.02

一个隐马尔科夫过程如下：
    
    - 1从初始概率生成当前隐藏状态
    - 2从隐藏状态生成当前词
    - 3从当前隐藏状态转移到下一个隐藏状态
    - 重复23，直至最后一个状态 
    
## 基本假设

- 齐次马尔科夫性假设：当前隐藏状态只依赖于前一个时刻的状态
- 观测独立性假设：任意时刻的观测结果只与该时刻的隐藏状态有关

## 三个基本问题

- 概率计算问题
    给定模型（π、A、B）和观测序列，计算该观测序列出现的概率
    
- 隐藏状态预测问题
    给定模型（π、A、B）和观测序列，计算最可能的隐藏状态序列
- 模型参数计算问题 
    给定观测序列，预测该模型的参数，是的生成该观测序列的概率最大
    
### 概率计算问题
    - 前向算法，就是假设有下一时刻的状态为i且观测结果是t的概率为前一时刻所有的隐藏状态每个单个的状态的概率乘上转移到i的概率之和
    - 然后与观测概率相乘(观测概率就是当前状态为i，观测序列为t的概率)
    - 例子：假设观测序列为o1,o2,o3,...,on
    - 定义a(t,i)为在o1,o2,o3...ot的观测序列下，状态为i
    - a(t+1,i)为在o1,o2,o3...ot+1的观测序列下，状态为i
    - 怎么求a(t+1,i)呢，求法就是a(t,j)*Aji对所有的j求和然后乘上Bi(t+1)
    - 依次迭代，求得结果

### 隐藏状态预测问题
    - 经典的维特比算法求解
        
### 模型参数计算问题
    - 监督学习，就是做统计，举个例子，当前状态为B，则下一个状态为E的概率是所有的当前为B的情况总和为分母，当前为B下一个状态为E的数量为分子，二者的商就是B->E的转移概率
    - 监督学习需要训练数据
    - 无监督学习，Baum-Welch方法（还没看懂）

## Trick

- 三阶马尔科夫并不很好的提高效果，倾向于生成新词