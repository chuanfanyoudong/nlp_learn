{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 结巴分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba # 导入结巴分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.cut at 0x0000016D40EAF5C8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"我来到天安门前，其实我感觉也很不错的\" # 输入文本\n",
    "jieba.cut(text) # 这样输出是一个生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from cache C:\\Users\\DELL-5~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.890 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['我', '来到', '天安门', '前', '，', '其实', '我', '感觉', '也', '很', '不错', '的']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 想转化成列表，有两种方法\n",
    "list(jieba.cut(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '来到', '天安门', '前', '，', '其实', '我', '感觉', '也', '很', '不错', '的']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 第二种方法\n",
    "# 使用lcut函数、其中lcut函数会把生成器转化成list\n",
    "jieba.lcut(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut函数的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def cut(self, sentence, cut_all=False, HMM=True):\n",
    "\n",
    "这是jieba.cut的源码\n",
    "\n",
    "可以看出，一共有三个参数：\n",
    "\n",
    "> sentence:输入的要分词的句子\n",
    "\n",
    "> cut_all:是否要全切分\n",
    "\n",
    "> HMM:是否适用隐马尔科夫进行新词识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '来到', '天安', '天安门', '门前', '', '', '其实', '我', '感觉', '也', '很', '不错', '的']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut_all指的是全切分，测试一波\n",
    "jieba.lcut(text, cut_all = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**全切分之后，会把所有的分词结果展示出来，例子中的天安、天安门、门前就是这个例子，这个可以应用在搜索领域**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用隐马尔可夫模型发现新词 ['他', '来到', '了', '网易', '杭研', '大厦']\n不使用隐马尔可夫模型发现新词 ['他', '来到', '了', '网易', '杭', '研', '大厦']\n"
     ]
    }
   ],
   "source": [
    "# HMM指的是是否是用隐马尔科夫模型发现新词、默认为True，测试一波\n",
    "text = \"他来到了网易杭研大厦\"\n",
    "print(\"使用隐马尔可夫模型发现新词\", jieba.lcut(text))\n",
    "print(\"不使用隐马尔可夫模型发现新词\", jieba.lcut(text, HMM = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们可以看到使用了隐马尔科夫之后，“杭研“被分出来了**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut函数的原理\n",
    "\n",
    "引用官方的解释：\n",
    "\n",
    "- 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)\n",
    "\n",
    "- 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合\n",
    "\n",
    "- 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于前缀词典生成有向无环图，这里后续我会，专门细数，现在给大家看一下有向无环图的结果**\n",
    "\n",
    ">下面是生成有向无环图的源码\n",
    "\n",
    "```python\n",
    "    def get_DAG(self, sentence):\n",
    "        self.check_initialized()\n",
    "        DAG = {}\n",
    "        N = len(sentence)\n",
    "        for k in xrange(N):\n",
    "            tmplist = []\n",
    "            i = k\n",
    "            frag = sentence[k]\n",
    "            while i < N and frag in self.FREQ:\n",
    "                if self.FREQ[frag]:\n",
    "                    tmplist.append(i)\n",
    "                i += 1\n",
    "                frag = sentence[k:i + 1]\n",
    "            if not tmplist:\n",
    "                tmplist.append(k)\n",
    "            DAG[k] = tmplist\n",
    "        return DAG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这里是有向无环图的示例:\n",
    "\n",
    "text = 我来到中国天安门\n",
    "\n",
    "**{0: [0], 1: [1, 2], 2: [2], 3: [3, 4], 4: [4], 5: [5, 6, 7], 6: [6], 7: [7]}**\n",
    "\n",
    "可以看出”我来到中国天安门“分别对应着标号是01234567\n",
    "\n",
    "上面的有向无环图的每一个键代表着第几个标号的字符、后面的list代表着该字符可以和哪些字符组成词语\n",
    "\n",
    "比如：5: [5, 6, 7], 可以看出 5对应着天，所以可以组成的词有 5-5：天；5-6：天安；5-7：天安门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合\n",
    "\n",
    "对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法\n",
    "\n",
    "**我会专门写一下这个的具体实现过程**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
